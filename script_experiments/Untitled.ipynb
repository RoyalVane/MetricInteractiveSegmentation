{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import collections\n",
    "import os\n",
    "import socket\n",
    "\n",
    "from datetime import datetime\n",
    "import timeit\n",
    "\n",
    "import deeptriplet.datasets\n",
    "import deeptriplet.metrics\n",
    "import deeptriplet.triplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import collections\n",
    "import os\n",
    "import socket\n",
    "\n",
    "from datetime import datetime\n",
    "import timeit\n",
    "\n",
    "import deeptriplet.datasets\n",
    "import deeptriplet.metrics\n",
    "import deeptriplet.triplet\n",
    "\n",
    "# Tensorboard include\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "gpu_id = 0\n",
    "\n",
    "nEpochs = 30  # Number of epochs for training\n",
    "resume_epoch = 0  # Default is 0, change if want to resume\n",
    "\n",
    "p = collections.OrderedDict()  # Parameters to include in report\n",
    "useTest = True  # See evolution of the test set when training\n",
    "nTestInterval = 1  # Run on test set every nTestInterval epochs\n",
    "snapshot = 5  # Store a model every snapshot epochs\n",
    "\n",
    "p['lr'] = 1e-3  # Learning rate\n",
    "p['wd'] = 5e-4  # Weight decay\n",
    "p['momentum'] = 0.9  # Momentum\n",
    "p['poly_power'] = 0.9\n",
    "\n",
    "save_dir_root = \"/srv/glusterfs/yardima/runs/deeplabv2/lfov-triplet/\"\n",
    "modelName = 'class-vgg-pascal'\n",
    "run_id = 0\n",
    "exp_name = 'pascal-deeplabv2-lfov-30dim-random-triplet'\n",
    "\n",
    "print('Using GPU: {} '.format(gpu_id))\n",
    "# Setting parameters\n",
    "\n",
    "\n",
    "save_dir = os.path.join(save_dir_root, 'run_' + str(run_id))\n",
    "\n",
    "\n",
    "########################## model\n",
    "\n",
    "net = deeptriplet.models.DeepLab_VGG()\n",
    "d = torch.load(\"/srv/glusterfs/yardima/runs/deeplabv2/lfov/run_5/models/class-vgg-pascal_epoch-29.pth\")\n",
    "del d['fc8.bias']\n",
    "del d['fc8.weight']\n",
    "net.load_state_dict(d, strict=False)\n",
    "net = net.eval().cuda()\n",
    "\n",
    "\n",
    "###########################  optimzers, datasets\n",
    "\n",
    "init_lr = p['lr']\n",
    "\n",
    "optimizer = optim.SGD([\n",
    "                {'params': net.get_parameter_group_v2(bias=False, final=False), 'lr': init_lr},\n",
    "                {'params': net.get_parameter_group_v2(bias=True, final=False), 'lr': init_lr*2, 'weight_decay':0},\n",
    "                {'params': net.get_parameter_group_v2(bias=False, final=True), 'lr': init_lr*10},\n",
    "                {'params': net.get_parameter_group_v2(bias=True, final=True), 'lr': init_lr*20, 'weight_decay':0}\n",
    "            ], lr=init_lr, momentum=p['momentum'], weight_decay=p['wd'])\n",
    "\n",
    "# loss_fn = nn.CrossEntropyLoss(ignore_index=255)\n",
    "loss_fn = deeptriplet.triplet.RandomTripletPreselected(n_batch=10, n_triplets=200)\n",
    "loss_fn_val = deeptriplet.triplet.RandomTripletPreselected(n_batch=1, n_triplets=200)\n",
    "\n",
    "valset = deeptriplet.datasets.PascalDatasetRandomTriplet(pascal_root=\"/scratch/yardima/datasets/voc12/VOCdevkit/VOC2012\",\n",
    "                                            split_file=\"/home/yardima/Python/experiments/pascal_split/val_obj.txt\",\n",
    "                                            n_triplets=200,\n",
    "                                            normalize_imagenet=True,\n",
    "                                            augment=False,\n",
    "                                            pad_zeros=True,\n",
    "                                            downsample_label=8)\n",
    "\n",
    "valloader = data.DataLoader(valset,\n",
    "                                batch_size=1,\n",
    "                                num_workers=2,\n",
    "                                shuffle=False)\n",
    "\n",
    "trainset = deeptriplet.datasets.PascalDatasetRandomTriplet(pascal_root=\"/scratch/yardima/datasets/voc12/VOCdevkit/VOC2012\",\n",
    "                        split_file=\"/home/yardima/Python/experiments/pascal_split/train_obj.txt\",\n",
    "                        n_triplets=200,\n",
    "                        normalize_imagenet=True,\n",
    "                        augment=True,\n",
    "                        pad_zeros=True,\n",
    "                        downsample_label=8,\n",
    "                        scale_low=0.8,\n",
    "                        scale_high=1.2)\n",
    "\n",
    "trainloader = data.DataLoader(trainset,\n",
    "                                batch_size=10,\n",
    "                                num_workers=4,\n",
    "                                shuffle=True)\n",
    "\n",
    "\n",
    "############################ custom functions\n",
    "\n",
    "def update_lr_poly(optimizer, init_lr, step, max_step, power):\n",
    "    lr = init_lr * ((1 - step / float(max_step)) ** (power))\n",
    "    optimizer.param_groups[0]['lr'] = lr\n",
    "    optimizer.param_groups[1]['lr'] = 2 * lr\n",
    "    optimizer.param_groups[2]['lr'] = 10 * lr\n",
    "    optimizer.param_groups[3]['lr'] = 20 * lr\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################## main script\n",
    "\n",
    "\n",
    "def generate_param_report(logfile, param):\n",
    "    log_file = open(logfile, 'w')\n",
    "    for key, val in param.items():\n",
    "        log_file.write(key + ':' + str(val) + '\\n')\n",
    "    log_file.close()\n",
    "\n",
    "\n",
    "if resume_epoch == 0:\n",
    "    print(\"Training from init...\")\n",
    "else:\n",
    "    print(\"Initializing weights from: {}...\".format(\n",
    "        os.path.join(save_dir, 'models', modelName + '_epoch-' + str(resume_epoch - 1) + '.pth')))\n",
    "    net.load_state_dict(\n",
    "        torch.load(os.path.join(save_dir, 'models', modelName + '_epoch-' + str(resume_epoch - 1) + '.pth'),\n",
    "                   map_location=lambda storage, loc: storage))  # Load all tensors onto the CPU\n",
    "\n",
    "if gpu_id >= 0:\n",
    "    torch.cuda.set_device(device=gpu_id)\n",
    "    net = net.cuda()\n",
    "\n",
    "\n",
    "\n",
    "if resume_epoch != nEpochs:\n",
    "    # Logging into Tensorboard\n",
    "    log_dir = os.path.join(save_dir, 'models', datetime.now().strftime('%b%d_%H-%M-%S') + '_' + socket.gethostname())\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    # Use the following optimizer\n",
    "    p['optimizer'] = str(optimizer)\n",
    "\n",
    "    generate_param_report(os.path.join(save_dir, exp_name + '.txt'), p)\n",
    "\n",
    "    num_img_tr = len(trainloader)\n",
    "    num_img_ts = len(valloader)\n",
    "    running_loss_tr = 0.0\n",
    "    global_step = 0\n",
    "    max_steps = nEpochs * num_img_tr\n",
    "    print(\"Training Network\")\n",
    "\n",
    "    # Main Training and Testing Loop\n",
    "    for epoch in range(resume_epoch, nEpochs):\n",
    "        start_time = timeit.default_timer()\n",
    "\n",
    "        net = net.eval()\n",
    "        for ii, sample_batched in enumerate(trainloader):\n",
    "\n",
    "            inputs, labels = sample_batched\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "            optimizer = update_lr_poly(optimizer, init_lr, global_step, max_steps, p['poly_power'])\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = net.forward(inputs)\n",
    "            loss = loss_fn.compute_loss(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss_tr += loss.item()\n",
    "\n",
    "\n",
    "            writer.add_scalar('data/total_loss_iter', loss.item(), global_step)\n",
    "            writer.add_scalar('data/lr', optimizer.param_groups[0]['lr'], global_step)\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "            del loss, outputs\n",
    "\n",
    "\n",
    "        running_loss_tr = running_loss_tr / num_img_tr\n",
    "        writer.add_scalar('data/total_loss_epoch', running_loss_tr, epoch)\n",
    "\n",
    "        print('[Epoch: %d]' % (epoch,))\n",
    "        print('Loss: %f' % running_loss_tr)\n",
    "        running_loss_tr = 0.\n",
    "        stop_time = timeit.default_timer()\n",
    "        print(\"Execution time: \" + str(stop_time - start_time) + \"\\n\")\n",
    "\n",
    "        # Save the model\n",
    "        if (epoch % snapshot) == snapshot - 1:\n",
    "            torch.save(net.state_dict(), os.path.join(save_dir, 'models', modelName + '_epoch-' + str(epoch) + '.pth'))\n",
    "            print(\"Save model at {}\\n\".format(\n",
    "                os.path.join(save_dir, 'models', modelName + '_epoch-' + str(epoch) + '.pth')))\n",
    "\n",
    "        # One testing epoch\n",
    "        if useTest and epoch % nTestInterval == (nTestInterval - 1):\n",
    "            net = net.eval()\n",
    "            count_val = 0\n",
    "            loss_val = 0.0\n",
    "            with torch.no_grad():\n",
    "                for ii, sample_batched in enumerate(valloader):\n",
    "                    inputs, labels = sample_batched\n",
    "                    inputs = inputs.cuda()\n",
    "                    labels = labels.cuda()\n",
    "                    \n",
    "                    outputs = net.forward(inputs)\n",
    "                    loss = loss_fn_val.compute_loss(outputs, labels)\n",
    "                    \n",
    "                    loss_val += loss.item()\n",
    "                    \n",
    "                    del outputs, loss\n",
    "                    \n",
    "                    count_val += 1\n",
    "            \n",
    "            loss_val /= count_val\n",
    "            \n",
    "            writer.add_scalar('data/val_loss', loss_val, epoch)\n",
    "            \n",
    "            ## net = net.train()\n",
    "\n",
    "    writer.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
